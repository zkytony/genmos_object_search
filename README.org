* sloop_object_search

SLOOP (Spatial Language Understanding Object-Oriented POMDP)
for Multi-Object Search.

** sloop object search package design decisions
   <<design_decisions>>
*** Objective (last updated 08/15/2022)
    sloop object search package is an object search package that:

     - [ ] Performs hierarchical object search with 3D local search and 2D global search
     - [ ] Allows specification of correlation between objects
     - [ ] Allows incremental update of the underlying search region
     - [ ] Permits the use of spatial language over the 2D global search region.
       - [ ] In fact, allows resolution of spatial language tuples incrementally,
             as unknown landmarks essentially serve as correlated objects.
             (A demo/experiment of this is more impressive)

*** gRPC calls as the core interface (last updated 08/15/2022)
    - sloop object search is a middleware-independent package who
      is intended to be run as a gRPC server, and offers its functionality
      through handling gRPC calls.

      As such, code in ~sloop_object_search~ is of course ROS-independent.

    - messages in gRPC should by default contain world-frame coordinates,
      while utility functions in grpc/utils perform conversion of those
      quantities to POMDP frames (or back). Quantities that are fed into
      POMDP models should by default be in POMDP frame.

*** Specification of Search Region (last updated 08/15/2022)
    2D search region, designed for global search, is fundamentally a occupancy grid map.
        this can be built from a point cloud or an occupancy grid map
    3D search region, designed for local search, is fundamentally an occupancy octree
        this can be built from a point cloud

*** Specification of Reachable Viewpoints (last updated 08/15/2022)
    There is no direct specification of reachable viewpoints.

    Instead, one could specify /unreachable/ viewpoints through:

    - Inference from the search region. all obstacles (2D) and occupied nodes (3D)
       are unreachable points.  <<reachable_viewpoints_design_1>>

    - Provide certain parameters in the configuration dict, such as "unreachable
      position inflation radius", and "planning region" (which limits the search
      space considered during planning)

*** Specification of (Primitive) Actions (last updated 08/15/2022)
    The global search agent's action space is based on topological graph nodes.
    The local search agent's action space is based on either relative moves,
       or a set of sampled viewpoints within the local region (like a topograph but fully connected)

    When the action is a viewpoint, it will respect the reachable viewpoints
    specification (see above)

*** Labels in GridMap2 (last updated 08/15/2022)
     Don't use labels there for the purpose of plain reachability. The
     obstacles set is sufficient for that - see design decision on reachable points
     ([[reachable_viewpoints_design_1]]).

     The inflation radius of obstacles can be a function in GridMap2 - it is
     simply a matter of adding more obstacles close to existing ones.

*** POMDP Agent Implementation (last updated 08/16/2022)
    - belief is handled by pomdp_py.OOBelief. There is no
      need for additional belief class.

    - ~MosAgent~ is the high-level class for a multi-object search agent.
      It does not assume if the world is 2D or 3D. The code is general.

      ~SloopMosAgent~ is a sloop agent whose underlying OOPOMDP is
      a MosAgent. Because MosAgent is not grounded to any world
      representation, I have left ~_init_oopomdp~ unimplemented.

    - We will implement: ~MosAgentBasic2D~, which is just a 2D local
      search agent (with primitive actions); There should be not
      much work in putting this one together. Will implement ~SloopMosAgentBasic2D~
      even though we will probably not use it for real robot experiments.

      ~MosAgentTopo2D~, which is an agent with topo map action space.
      This should also not be too much work. With that, I can implement
      the ~SloopMosAgentTopo2D~, which is our global search agent.

    - Parameters for sensors are in metric units when being specified
      by the user (for a real robot scenario), but converted into POMDP
      coordinate units when passed in for creating an agent. This conversion
      is handled by us.


*** ProcessObservation vs. UpdateSearchRegion (last updated 08/20/2022)
    Even though both are related to updating the agent's model of
    the world (and/or the belief), we will separate them into two
    rpc methods. This clarifies and simplifies the implementation,
    as both are concerned with quite distinct issues and are called
    likely in different frequency.
*** Visualization functions in ros_utils
    Visualization functions in ~ros_utils.py~ that begin with ~make_*~ should be
    general in the sense that it doesn't rely on, for example, the assumption in
    the OOPOMDP camera model which by-default looks at -z while ROS by-default
    looks at +x.  These functions just return visualization markers for the
    poses and headers that are given, and don't make any assumption about what
    frames those poses are for. Users of those functions should carefully pass
    in the appropriate header and pose - *what is visualized is what you pass in*.

    Visualization functions that do not begin with ~make_*~ do not follow the
    above convention. Example ~viz_msgs_for_robot_pose_proto~ will return RVIZ
    marker and tf2 message that account for the default rotation differences
    between the camera in ROS and the camera in SLOOP.
*** Belief over robot pose
    The POMDP agent allows the robot to have uncertainty over its pose.
    It expects a localization module on the system will output estimates
    about the robot pose, with uncertainty represented by a covariance
    matrix. Therefore, the POMDP agent will model the belief over robot
    pose as a Gaussian. During planning, the POMDP agent samples robot
    poses from this belief to run MCTS, and these samples will be used
    /as is/ as observations about the robot pose.

*** Coordinate Frames
    The POMDP agent uses a discretized coordinate frame, where
    the coordinates are integers (could be positive or negative).
    This frame is translated and scaled with respect to the world
    frame. There is no rotation difference between them.

    Code inside sloop_object_search/oopomdp are by default working with
    POMDP frame (except for, e.g. SearchRegion which connects the two).
    Code inside sloop_object_search/grpc assume client and server communicate
    with messages that by default contain coordinates in the world frame.
*** Object detection precision in POMDP frame
    Although ~pomdp_detection_from_proto~ allows specifying position
    and rotation precisions when converting object detection from
    the world frame to the POMDP frame, we do not provide a way
    to configure those precisions because the default settings
    is already appropriate for the POMDP model (positions are
    integers, and rotation/size precision to 0.001 is fine-grained).
*** Action id and planning
    Each ~PlanActionReply~ contains an 'action_id', which is used to:
    - Inform the server that the action execution has finished
    - Label a ProcessObservationRequest to be related to the action.
*** ObjectDetection, Voxel and ObjectVoxel
    regarding the observation of objects in the 3D object search model,
    there are three types. ObjectDetection is what the robot would
    receive (i.e. what the grpc server receives). Voxel is used when
    building a volumetric observation from a set of object detections.
    ObjectVoxel is specific to an object $i$, used to refer to a voxel
    in $V_i$, the field of view of object i. This is used during planning
    and updating the planner.
*** Object detection types
    We can handle:
    - 3D object detection bounding boxes. If not available:
    - label-only object detection.

*** Server and Client: A Concrete Use Case
    The server runs planner, holds agent's beliefs, etc.
    So ideally, the server is a powerful machine. It is
    likely for the server to be remote.

    The client talks to the server. It also interacts
    with the robot - the server doesn't do that - the
    server just cares about POMDP stuff.

    So, you can imagine, setting up the sloop_object_search
    server on your static desktop machine with good hardware,
    and running the client on a laptop that you carry when
    you have a mission with the robot.

    You will be able to visualize the necessary things to
    know what's going on in planning and in belief state.
    That's the intended use case scenario for this package.
*** Visualization
    <<slp-visualization>>
    The client wants to know what's going on. The client may
    not use RViZ.

    Visualization involved in sloop_object_search is in
    the following aspects:
    - visualize the search region (both local and global)
    - visualize the belief state (local and global)
    - visualize the plan or planned action
    - visualize the planning process
    - visualize the FOV and observations


** Usage

*** Launch the sloop_object_search gRPC server
#+begin_src
python -m sloop_object_search.grpc.server
#+end_src


** Set up sloop_object_search for ROS

1. Go to the 'src' folder of your ROS workspace.
2. Create a symbolic link to the `sloop_object_search/ros` folder, and name that
   symbolic link "sloop_object_search." That is,
   #+begin_src
   ln -s /path/to/sloop_object_search/ros sloop_object_search_ros
   #+end_src

3. Compile the package
   #+begin_src
   catkin_make -DCATKLIN_WHITELIST_PACKAGES="sloop_object_search_ros"
   #+end_src
